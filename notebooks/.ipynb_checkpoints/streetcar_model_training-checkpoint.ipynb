{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streetcar Delay Prediction - Deep Learning - REFACTORED\n",
    "\n",
    "GOAL: predict streetcar delays using a simple Keras model\n",
    "\n",
    "Refactored to look at delays by hour by day by route by direction\n",
    "\n",
    "Source dataset: : https://open.toronto.ca/dataset/ttc-streetcar-delay-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links to key parts of the notebook <a name='linkanchor' />\n",
    "<a href=#ingestdash>Ingest data</a>\n",
    "\n",
    "<a href=#definecategories>Define feature categories</a>\n",
    "\n",
    "<a href=#bookmark>Deal with missing values</a>\n",
    "\n",
    "<a href=#modelfit>Define and fit model</a>\n",
    "\n",
    "<a href=#reload>Reload saved model and weights</a>\n",
    "\n",
    "<a href=#confusionmatrix>Confusion matrix</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common imports and global variable definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "# import datetime, timedelta\n",
    "import datetime\n",
    "import pydotplus\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dateutil import relativedelta\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "# DSX code to import uploaded documents\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import sys\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "#model libraries\n",
    "from tensorflow.keras.metrics import Accuracy, Recall, Precision\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#from tf.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "#import datetime\n",
    "#from datetime import date\n",
    "from sklearn import metrics\n",
    "# import pipeline libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from custom_classes import encode_categorical\n",
    "from custom_classes import prep_for_keras_input\n",
    "from custom_classes import fill_empty\n",
    "from custom_classes import encode_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is: C:\\personal\\manning\\deep_learning_for_structured_data\\notebooks\n",
      "path_to_yaml C:\\personal\\manning\\deep_learning_for_structured_data\\notebooks\\streetcar_model_training_config.yml\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "current_path = os.getcwd()\n",
    "print(\"current directory is: \"+current_path)\n",
    "\n",
    "path_to_yaml = os.path.join(current_path, 'streetcar_model_training_config.yml')\n",
    "print(\"path_to_yaml \"+path_to_yaml)\n",
    "try:\n",
    "    with open (path_to_yaml, 'r') as c_file:\n",
    "        config = yaml.safe_load(c_file)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date today 2020-04-15 21:58:52.240858\n",
      "start date 2014-01-01\n",
      "end date 2019-12-31\n"
     ]
    }
   ],
   "source": [
    "# load parameters\n",
    "\n",
    "testproportion = config['test_parms']['testproportion'] # proportion of data reserved for test set\n",
    "trainproportion = config['test_parms']['trainproportion'] # proportion of non-test data dedicated to training (vs. validation)\n",
    "verboseout = config['general']['verboseout']\n",
    "includetext = config['general']['includetext'] # switch to determine whether text columns are included in the model\n",
    "save_model_plot = config['general']['save_model_plot'] # switch to determine whether to generate plot with plot_model\n",
    "tensorboard_callback = config['general']['tensorboard_callback'] # switch to determine if tensorboard callback defined\n",
    "\n",
    "presaved = config['general']['presaved']\n",
    "savemodel = config['general']['savemodel']\n",
    "picklemodel = config['general']['picklemodel']\n",
    "hctextmax = config['general']['hctextmax']\n",
    "maxwords = config['general']['maxwords']\n",
    "textmax = config['general']['textmax']\n",
    "\n",
    "targetthresh = config['general']['targetthresh']\n",
    "targetcontinuous = config['general']['targetcontinuous']\n",
    "\n",
    "#time of day thresholds\n",
    "time_of_day = {'overnight':{'start':0,'end':5},'morning_rush':{'start':5,'end':10},\n",
    "              'midday':{'start':10,'end':15},'aft_rush':{'start':15,'end':19},'evening':{'start':19,'end':24}}\n",
    "\n",
    "\n",
    "\n",
    "emptythresh = config['general']['emptythresh']\n",
    "zero_weight = config['general']['zero_weight']\n",
    "one_weight = config['general']['one_weight']\n",
    "one_weight_offset = config['general']['one_weight_offset']\n",
    "patience_threshold = config['general']['patience_threshold']\n",
    "\n",
    "\n",
    "# modifier for saved model elements\n",
    "modifier = config['general']['modifier']\n",
    "\n",
    "# control whether training controlled by early stop\n",
    "early_stop = True\n",
    "\n",
    "# default hyperparameter values\n",
    "learning_rate = config['hyperparameters']['learning_rate']\n",
    "dropout_rate = config['hyperparameters']['dropout_rate']\n",
    "l2_lambda = config['hyperparameters']['l2_lambda']\n",
    "loss_func = config['hyperparameters']['loss_func']\n",
    "output_activation = config['hyperparameters']['output_activation']\n",
    "batch_size = config['hyperparameters']['batch_size']\n",
    "epochs = config['hyperparameters']['epochs']\n",
    "\n",
    "# date values\n",
    "date_today = datetime.now()\n",
    "print(\"date today\",date_today)\n",
    "start_date =  date(config['general']['start_year'],config['general']['start_month'], config['general']['start_day'])\n",
    "print(\"start date\",start_date)\n",
    "end_date = date(config['general']['end_year'],config['general']['end_month'], config['general']['end_day'])\n",
    "print(\"end date\",end_date)\n",
    "\n",
    "\n",
    "# pickled original dataset and post-preprocessing dataset\n",
    "pickled_data_file = config['general']['pickled_data_file']\n",
    "pickled_dataframe = config['general']['pickled_dataframe']\n",
    "routedirection_file = config['general']['route_direction_file']\n",
    "\n",
    "# experiment parameter\n",
    "\n",
    "current_experiment = config['test_parms']['current_experiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_of_day = {'overnight':{'start':0,'end':5},'morning_rush':{'start':5,'end':10},\n",
    "#              'midday':{'start':10,'end':15},'aft_rush':{'start':15,'end':19},'evening':{'start':19,'end':23}}\n",
    "\n",
    "\n",
    "def get_time(hour):\n",
    "    for tod in time_of_day:\n",
    "        if (hour >= time_of_day[tod]['start']) and (hour < time_of_day[tod]['end']):\n",
    "            tod_out = tod\n",
    "    return(tod_out)\n",
    "\n",
    "def weekend_time(day, tod):\n",
    "    if (day=='Saturday') or (day=='Sunday'):\n",
    "        return('w'+tod)\n",
    "    else:\n",
    "        return(tod)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths required\n",
    "\n",
    "def get_path():\n",
    "    '''get the path for data files'''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n",
    "    return(path)\n",
    "\n",
    "def get_pipeline_path():\n",
    "    '''get the path for data files'''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'pipelines'))\n",
    "    return(path)\n",
    "\n",
    "def get_model_path():\n",
    "    '''get the path for data files'''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'models'))\n",
    "    return(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_experiment_parameters(experiment_number, count_no_delay, count_delay):\n",
    "    ''' set the appropriate parameters for the experiment '''\n",
    "    print(\"setting parameters for experiment \", experiment_number)\n",
    "    # default settings for early stopping:\n",
    "    es_monitor = \"val_loss\"\n",
    "    es_mode = \"min\"\n",
    "    if experiment_number == 0:\n",
    "        #\n",
    "        early_stop = False\n",
    "        #\n",
    "        one_weight = 1.0\n",
    "        #\n",
    "        epochs = 1\n",
    "    elif experiment_number == 1:\n",
    "        #\n",
    "        early_stop = False\n",
    "        #\n",
    "        one_weight = 1.0\n",
    "        #\n",
    "        epochs = 10\n",
    "    elif experiment_number == 2:\n",
    "        #\n",
    "        early_stop = False\n",
    "        #\n",
    "        one_weight = 1.0\n",
    "        #\n",
    "        epochs = 50\n",
    "    elif experiment_number == 3:\n",
    "        #\n",
    "        early_stop = False\n",
    "        #\n",
    "        one_weight = (count_no_delay/count_delay) + one_weight_offset\n",
    "        #\n",
    "        epochs = 50\n",
    "    elif experiment_number == 4:\n",
    "        #\n",
    "        early_stop = True\n",
    "        es_monitor = \"val_loss\"\n",
    "        es_mode = \"min\"\n",
    "        #\n",
    "        one_weight = (count_no_delay/count_delay) + one_weight_offset\n",
    "        #\n",
    "        epochs = 50\n",
    "    elif experiment_number == 5:\n",
    "        #\n",
    "        early_stop = True\n",
    "        # if early stopping fails because the level of TensorFlow/Python, comment out the following\n",
    "        # line and uncomment the subsequent if statement\n",
    "        es_monitor=\"val_accuracy\"\n",
    "        '''\n",
    "        if sys.version_info >= (3,7):\n",
    "            es_monitor=\"val_accuracy\"\n",
    "        else:\n",
    "            es_monitor = \"val_acc\"\n",
    "        '''\n",
    "        es_mode = \"max\"\n",
    "        #\n",
    "        one_weight = (count_no_delay/count_delay) + one_weight_offset\n",
    "        #\n",
    "        epochs = 50\n",
    "    else:\n",
    "        early_stop = True\n",
    "    return(early_stop, one_weight, epochs,es_monitor,es_mode)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest data and create refactored dataframe\n",
    "- Ingest data for route information and delay information\n",
    "- Create refactored dataframe with one row per route / direction / timeslot combination\n",
    "\n",
    "<a name='ingestdash' />\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of valid routes and directions into dataframe\n",
    "def ingest_data(path):\n",
    "    routedirection_frame = pd.read_csv(os.path.join(path,routedirection_file))\n",
    "    routedirection_frame.tail()\n",
    "    file_name = os.path.join(path,pickled_dataframe)\n",
    "    merged_data = pd.read_pickle(file_name)\n",
    "    merged_data.head()\n",
    "    return(routedirection_frame, merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add derived columns to merged_data dataframe\n",
    "def prep_merged_data(merged_data):\n",
    "    # define cols for year month day hour\n",
    "    merged_data['year'] = pd.DatetimeIndex(merged_data['Report Date']).year\n",
    "    merged_data['month'] = pd.DatetimeIndex(merged_data['Report Date']).month\n",
    "    merged_data['daym'] = pd.DatetimeIndex(merged_data['Report Date']).day\n",
    "    merged_data['hour'] = pd.DatetimeIndex(merged_data['Report Date Time']).hour\n",
    "    # define time of day column\n",
    "    merged_data['time_of_day'] = merged_data['hour'].apply(lambda x:get_time(x))\n",
    "    # add a special timeframe for weekends\n",
    "    merged_data['time_of_day'] = merged_data.apply(lambda x: weekend_time(x['Day'], x['time_of_day']), axis=1)\n",
    "    if targetcontinuous:\n",
    "        merged_data['target'] = merged_data['Min Delay']\n",
    "    else:\n",
    "        merged_data['target'] = np.where(merged_data['Min Delay'] >= targetthresh, 1, 0 )\n",
    "    return(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe containing rows for each timeslot for each route for each direction\n",
    "# and merge with the input merged_data dataframe to get a result of a sparse dataframe with the\n",
    "# timeslot / route / direction combinations where delays occurred\n",
    "def prep_sparse_df(routedirection_frame, merged_data):\n",
    "    routedirection_frame['count'] = 0\n",
    "    print(\"routedirection\")\n",
    "    display(routedirection_frame[:5])\n",
    "    # define a dataframe with a row for each date to be covered\n",
    "    days = pd.date_range(start_date, end_date, freq='D')\n",
    "    date_frame = pd.DataFrame({'date':days,'count':0})\n",
    "    print(\"date_frame\")\n",
    "    display(date_frame[:5])\n",
    "    # define a dataframe with a row for each hour\n",
    "    hour_list = list(range(0,24))\n",
    "    hour_frame = pd.DataFrame({'hour':hour_list,'count':0})\n",
    "    print(\"hour_frame\")\n",
    "    display(hour_frame[:5])\n",
    "    #vprint(hour_frame.head())\n",
    "    # merge date_frame and routedirection\n",
    "    result1 = pd.merge(date_frame, routedirection_frame, on='count', how='outer')\n",
    "    print(\"result1\")\n",
    "    display(result1[:5])\n",
    "    # merge result1 with hour_frame\n",
    "    result2 = pd.merge(result1, hour_frame, on='count', how='outer')\n",
    "    result2 = result2.rename(columns={'date': 'Report Date'})\n",
    "    result2.Route = result2.Route.astype(str)\n",
    "    # segment the date\n",
    "    result2['year'] = pd.DatetimeIndex(result2['Report Date']).year\n",
    "    result2['month'] = pd.DatetimeIndex(result2['Report Date']).month\n",
    "    result2['daym'] = pd.DatetimeIndex(result2['Report Date']).day\n",
    "    result2['day'] = pd.DatetimeIndex(result2['Report Date']).weekday\n",
    "    print(\"result2\")\n",
    "    display(result2[:5])\n",
    "    print(\"merged_data before\")\n",
    "    display(merged_data[:5])\n",
    "    # drop extraneous columns from merged_data\n",
    "    merged_data = merged_data.drop(['Time',\n",
    "     'Report Date Time',\n",
    "     'year',\n",
    "     'month',\n",
    "     'daym',\n",
    "     'time_of_day','Min Gap','Location','Incident','Vehicle','target','Day'],axis=1)\n",
    "    print(\"merged_data after dropping extraneous columns\")\n",
    "    display(merged_data[:5])\n",
    "    # join result2 and the trimmed merged_data\n",
    "    result3 = pd.merge(result2,merged_data ,how='left', on=['Report Date','Route','Direction','hour'])\n",
    "    result3['Min Delay'].fillna(value=0.0,inplace=True)\n",
    "    result3['target'] = np.where(result3['Min Delay'] > 0.0, 1, 0 )\n",
    "    print(\"result3\")\n",
    "    display(result3[:5])\n",
    "    return(result3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFACTORED DATAFRAME SHOULD HAVE THE FOLLOWING COLUMNS:\n",
    "# DAY - for every day in the history from Jan 1 2014 to July 31 2018\n",
    "# HOUR - for every hour of the day\n",
    "#  for 501, regular route 5:00 am - midnight; 301 overnight\n",
    "#   for 503: 7- 10:00 am; 4-7:00 pm\n",
    "# for 504 5:00 am - 2:00 am; 304 overnight\n",
    "# for 505 5:00 am - 1:00 am\n",
    "# for 506 5:00 am - 1:00 am; 306 overnight\n",
    "# for 509 5:00 am - 1:00 am\n",
    "# for 510 5:00 am - 2:00 am; 310 overnight\n",
    "# for 511 5:00 am - 1:00 am\n",
    "# for 512 5:00 am - 2:00 am\n",
    "# for 514 (Cherry street)\n",
    "# ROUTE\n",
    "# DIRECTION\n",
    "# DELAY - where this could be count OR duration OR binary\n",
    "\n",
    "# example of filling in values:\n",
    "# data['PriceDate'] =  pd.to_datetime(data['PriceDate'], format='%m/%d/%Y')\n",
    "# data = data.sort_values(by=['PriceDate'], ascending=[True])\n",
    "# data.set_index('PriceDate', inplace=True)\n",
    "# print (data)\n",
    "\n",
    "# data = data.resample('D').ffill().reset_index()\n",
    "# print (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Prep Calls\n",
    "Contains calls to functions to load data, prep input dataframes, and create refactored dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path is C:\\personal\\manning\\deep_learning_for_structured_data\\data\n",
      "shape of pre refactored dataset (61500, 17)\n",
      "routedirection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Route</th>\n",
       "      <th>Direction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>301</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Route Direction  count\n",
       "0    301         e      0\n",
       "1    301         w      0\n",
       "2    301         b      0\n",
       "3    304         e      0\n",
       "4    304         e      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_frame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  count\n",
       "0 2014-01-01      0\n",
       "1 2014-01-02      0\n",
       "2 2014-01-03      0\n",
       "3 2014-01-04      0\n",
       "4 2014-01-05      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour_frame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour  count\n",
       "0     0      0\n",
       "1     1      0\n",
       "2     2      0\n",
       "3     3      0\n",
       "4     4      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "      <th>Route</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  count  Route Direction\n",
       "0 2014-01-01      0    301         e\n",
       "1 2014-01-01      0    301         w\n",
       "2 2014-01-01      0    301         b\n",
       "3 2014-01-01      0    304         e\n",
       "4 2014-01-01      0    304         e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report Date</th>\n",
       "      <th>count</th>\n",
       "      <th>Route</th>\n",
       "      <th>Direction</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>daym</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Report Date  count Route Direction  hour  year  month  daym  day\n",
       "0  2014-01-01      0   301         e     0  2014      1     1    2\n",
       "1  2014-01-01      0   301         e     1  2014      1     1    2\n",
       "2  2014-01-01      0   301         e     2  2014      1     1    2\n",
       "3  2014-01-01      0   301         e     3  2014      1     1    2\n",
       "4  2014-01-01      0   301         e     4  2014      1     1    2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_data before\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Incident</th>\n",
       "      <th>Location</th>\n",
       "      <th>Min Delay</th>\n",
       "      <th>Min Gap</th>\n",
       "      <th>Report Date</th>\n",
       "      <th>Route</th>\n",
       "      <th>Time</th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Report Date Time</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>daym</th>\n",
       "      <th>hour</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02 06:31:31</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>e</td>\n",
       "      <td>Late Leaving Garage</td>\n",
       "      <td>dundas and roncesvalles</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>505</td>\n",
       "      <td>06:31:00</td>\n",
       "      <td>4018</td>\n",
       "      <td>2014-01-02 06:31:31</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>morning_rush</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 12:43:43</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>e</td>\n",
       "      <td>Utilized Off Route</td>\n",
       "      <td>king and shaw</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>12:43:00</td>\n",
       "      <td>4128</td>\n",
       "      <td>2014-01-02 12:43:43</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>midday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 14:01:01</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>w</td>\n",
       "      <td>Held By</td>\n",
       "      <td>bingham and kingston road</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>501</td>\n",
       "      <td>14:01:00</td>\n",
       "      <td>4016</td>\n",
       "      <td>2014-01-02 14:01:01</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>midday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 14:22:22</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>w</td>\n",
       "      <td>Investigation</td>\n",
       "      <td>king st. and roncesvalles</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>14:22:00</td>\n",
       "      <td>4175</td>\n",
       "      <td>2014-01-02 14:22:22</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>midday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 16:42:42</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>e</td>\n",
       "      <td>Utilized Off Route</td>\n",
       "      <td>bathurst and king</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>16:42:00</td>\n",
       "      <td>4080</td>\n",
       "      <td>2014-01-02 16:42:42</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>aft_rush</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Day Direction             Incident  \\\n",
       "Report Date Time                                               \n",
       "2014-01-02 06:31:31  Thursday         e  Late Leaving Garage   \n",
       "2014-01-02 12:43:43  Thursday         e   Utilized Off Route   \n",
       "2014-01-02 14:01:01  Thursday         w              Held By   \n",
       "2014-01-02 14:22:22  Thursday         w        Investigation   \n",
       "2014-01-02 16:42:42  Thursday         e   Utilized Off Route   \n",
       "\n",
       "                                      Location  Min Delay  Min Gap  \\\n",
       "Report Date Time                                                     \n",
       "2014-01-02 06:31:31    dundas and roncesvalles        4.0      8.0   \n",
       "2014-01-02 12:43:43              king and shaw       20.0     22.0   \n",
       "2014-01-02 14:01:01  bingham and kingston road       13.0     19.0   \n",
       "2014-01-02 14:22:22  king st. and roncesvalles        7.0     11.0   \n",
       "2014-01-02 16:42:42          bathurst and king        3.0      6.0   \n",
       "\n",
       "                    Report Date Route      Time Vehicle    Report Date Time  \\\n",
       "Report Date Time                                                              \n",
       "2014-01-02 06:31:31  2014-01-02   505  06:31:00    4018 2014-01-02 06:31:31   \n",
       "2014-01-02 12:43:43  2014-01-02   504  12:43:00    4128 2014-01-02 12:43:43   \n",
       "2014-01-02 14:01:01  2014-01-02   501  14:01:00    4016 2014-01-02 14:01:01   \n",
       "2014-01-02 14:22:22  2014-01-02   504  14:22:00    4175 2014-01-02 14:22:22   \n",
       "2014-01-02 16:42:42  2014-01-02   504  16:42:00    4080 2014-01-02 16:42:42   \n",
       "\n",
       "                     year  month  daym  hour   time_of_day  target  \n",
       "Report Date Time                                                    \n",
       "2014-01-02 06:31:31  2014      1     2     6  morning_rush       0  \n",
       "2014-01-02 12:43:43  2014      1     2    12        midday       1  \n",
       "2014-01-02 14:01:01  2014      1     2    14        midday       1  \n",
       "2014-01-02 14:22:22  2014      1     2    14        midday       1  \n",
       "2014-01-02 16:42:42  2014      1     2    16      aft_rush       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_data after dropping extraneous columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>Min Delay</th>\n",
       "      <th>Report Date</th>\n",
       "      <th>Route</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02 06:31:31</th>\n",
       "      <td>e</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>505</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 12:43:43</th>\n",
       "      <td>e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 14:01:01</th>\n",
       "      <td>w</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>501</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 14:22:22</th>\n",
       "      <td>w</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 16:42:42</th>\n",
       "      <td>e</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Direction  Min Delay Report Date Route  hour\n",
       "Report Date Time                                                \n",
       "2014-01-02 06:31:31         e        4.0  2014-01-02   505     6\n",
       "2014-01-02 12:43:43         e       20.0  2014-01-02   504    12\n",
       "2014-01-02 14:01:01         w       13.0  2014-01-02   501    14\n",
       "2014-01-02 14:22:22         w        7.0  2014-01-02   504    14\n",
       "2014-01-02 16:42:42         e        3.0  2014-01-02   504    16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report Date</th>\n",
       "      <th>count</th>\n",
       "      <th>Route</th>\n",
       "      <th>Direction</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>daym</th>\n",
       "      <th>day</th>\n",
       "      <th>Min Delay</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>e</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Report Date  count Route Direction  hour  year  month  daym  day  Min Delay  \\\n",
       "0  2014-01-01      0   301         e     0  2014      1     1    2        0.0   \n",
       "1  2014-01-01      0   301         e     1  2014      1     1    2        0.0   \n",
       "2  2014-01-01      0   301         e     2  2014      1     1    2        0.0   \n",
       "3  2014-01-01      0   301         e     3  2014      1     1    2        0.0   \n",
       "4  2014-01-01      0   301         e     4  2014      1     1    2        0.0   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of refactored dataset (2952587, 11)\n",
      "count of no delay  2892594\n",
      "count of delay  59993\n",
      "setting parameters for experiment  2\n",
      "early_stop is  False\n",
      "one_weight is  1.0\n",
      "epochs is  50\n",
      "es_monitor is  val_loss\n",
      "es_mode is  min\n"
     ]
    }
   ],
   "source": [
    "# master calls\n",
    "# get the path for data files\n",
    "path = get_path()\n",
    "print(\"path is\",path)\n",
    "# load route direction and delay data datframes\n",
    "directions_df, merged_data = ingest_data(path)\n",
    "merged_data = prep_merged_data(merged_data)\n",
    "print(\"shape of pre refactored dataset\", merged_data.shape)\n",
    "merged_data['year'].value_counts()\n",
    "merged_data.groupby(['Route','Direction']).size().reset_index().rename(columns={0:'count'}).tail(50)\n",
    "# create refactored dataframe with one row for each route / direction / timeslot combination\n",
    "merged_data = prep_sparse_df(directions_df, merged_data)\n",
    "print(\"shape of refactored dataset\", merged_data.shape)\n",
    "count_no_delay = merged_data[merged_data['target']==0].shape[0]\n",
    "count_delay = merged_data[merged_data['target']==1].shape[0]\n",
    "print(\"count of no delay \",count_no_delay)\n",
    "print(\"count of delay \",count_delay)\n",
    "# define parameters for the current experiment\n",
    "experiment_number = current_experiment\n",
    "early_stop, one_weight, epochs,es_monitor,es_mode = set_experiment_parameters(experiment_number, count_no_delay, count_delay)\n",
    "print(\"early_stop is \",early_stop)\n",
    "print(\"one_weight is \",one_weight)\n",
    "print(\"epochs is \",epochs)\n",
    "print(\"es_monitor is \",es_monitor)\n",
    "print(\"es_mode is \",es_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2952587, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define test / training sets; encode categorical values; process text field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and test data set\n",
    "def get_train_validation_test(dataset):\n",
    "    train, test = train_test_split(dataset, test_size = testproportion)\n",
    "    dtrain, dvalid = train_test_split(train, random_state=123, train_size=trainproportion)\n",
    "    print(\"Through train test split. Test proportion:\")\n",
    "    print(testproportion)\n",
    "    return(dtrain,dvalid,test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define feature categories <a name='definecategories' />\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cols ['Report Date', 'count', 'Route', 'Direction', 'hour', 'year', 'month', 'daym', 'day', 'Min Delay', 'target']\n"
     ]
    }
   ],
   "source": [
    "allcols = list(merged_data)\n",
    "print(\"all cols\",allcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the required column lists\n",
    "def def_col_lists():\n",
    "    textcols = [] # columns to deal with as text - replace entries with multiple IDs and use embeddings, RNN\n",
    "    continuouscols = [] # columns to deal with as continuous values - no embeddings\n",
    "    if targetcontinuous:\n",
    "        excludefromcolist = ['count','Report Date', 'target','count_md','Min Delay'] # columns to exclude completely from the model\n",
    "\n",
    "    else:\n",
    "        # if target column is not renamed Min Delay put Min Delay in exclusion list\n",
    "        excludefromcolist = ['count','Report Date', 'target','count_md', 'Min Delay'] # columns to exclude completely from the model\n",
    "    nontextcols = list(set(allcols) - set(textcols))\n",
    "    collist = list(set(nontextcols) - set(excludefromcolist) - set(continuouscols))\n",
    "    for col in continuouscols:\n",
    "        print(\"col is\",col)\n",
    "        merged_data[col] = merged_data[col].astype(float)\n",
    "        print(\"got through one\")\n",
    "        superset_data[col] = superset_data[col].astype(float)\n",
    "    # print column list lengths and contents:\n",
    "    print(\"allcols\",len(allcols))\n",
    "    print(\"excludefromcolist\",len(excludefromcolist))\n",
    "    print(excludefromcolist)\n",
    "    print(\"textcols\",len(textcols))\n",
    "    print(textcols)\n",
    "    print(\"continuouscols\",len(continuouscols))\n",
    "    print(continuouscols)\n",
    "    print(\"collist\",len(collist))\n",
    "    print(collist)\n",
    "    return(collist,continuouscols,textcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define keras variables\n",
    "\n",
    "\n",
    "# X for the features used\n",
    "\n",
    "def get_keras_vars(dataset):\n",
    "    X = {}\n",
    "    dictlist = []\n",
    "    for col in collist:\n",
    "        if verboseout:\n",
    "            print(\"cat col is\",col)\n",
    "        X[col] = np.array(dataset[col])\n",
    "        dictlist.append(np.array(dataset[col]))\n",
    "\n",
    "    for col in textcols:\n",
    "        if verboseout:\n",
    "            print(\"text col is\",col)\n",
    "        X[col] = pad_sequences(dataset[col], maxlen=max_dict[col])\n",
    "        dictlist.append(pad_sequences(dataset[col], maxlen=max_dict[col]))\n",
    "\n",
    "    for col in continuouscols:\n",
    "        if verboseout:\n",
    "            print(\"cont col is\",col)\n",
    "        X[col] = np.array(dataset[col])\n",
    "        dictlist.append(np.array(dataset[col]))\n",
    "\n",
    "    return X, dictlist\n",
    "\n",
    "def get_keras_list_only(X_in):\n",
    "    dictlist = []\n",
    "    for key, value in X_in.items():\n",
    "        print(\"X def loop key\",key)\n",
    "        print(\"value shape\",value.shape)\n",
    "        temp = [key,value]\n",
    "        dictlist.append(value)\n",
    "    return dictlist\n",
    "\n",
    "def get_keras_np(X_in):\n",
    "    return np.array(list(X_in.items()),dtype=object)\n",
    "# np.array(list(result.items()), dtype=dtype)\n",
    "\n",
    "# the deployment API for Watson Studio can only take a list/array, not a dictionary, so define list-only version for input\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allcols 11\n",
      "excludefromcolist 5\n",
      "['count', 'Report Date', 'target', 'count_md', 'Min Delay']\n",
      "textcols 0\n",
      "[]\n",
      "continuouscols 0\n",
      "[]\n",
      "collist 7\n",
      "['daym', 'Direction', 'Route', 'hour', 'month', 'year', 'day']\n",
      "fill empty xform\n",
      "col is  daym\n",
      "col is  Direction\n",
      "col is  Route\n",
      "col is  hour\n",
      "col is  month\n",
      "col is  year\n",
      "col is  day\n",
      "transform col is  daym\n",
      "after transform col is  daym\n",
      "transform col is  Direction\n",
      "after transform col is  Direction\n",
      "transform col is  Route\n",
      "after transform col is  Route\n",
      "transform col is  hour\n",
      "after transform col is  hour\n",
      "transform col is  month\n",
      "after transform col is  month\n",
      "transform col is  year\n",
      "after transform col is  year\n",
      "transform col is  day\n",
      "after transform col is  day\n",
      "Through train test split. Test proportion:\n",
      "0.2\n",
      "cat col is daym\n",
      "cat col is Direction\n",
      "cat col is Route\n",
      "cat col is hour\n",
      "cat col is month\n",
      "cat col is year\n",
      "cat col is day\n",
      "cat col is daym\n",
      "cat col is Direction\n",
      "cat col is Route\n",
      "cat col is hour\n",
      "cat col is month\n",
      "cat col is year\n",
      "cat col is day\n",
      "cat col is daym\n",
      "cat col is Direction\n",
      "cat col is Route\n",
      "cat col is hour\n",
      "cat col is month\n",
      "cat col is year\n",
      "cat col is day\n",
      "keras variables defined\n",
      "X_train_list [array([ 7, 17,  4, ...,  8, 10, 17], dtype=int64), array([4, 3, 4, ..., 1, 1, 3]), array([ 2,  3, 12, ..., 10, 10,  7]), array([12, 13, 12, ..., 22, 15, 23], dtype=int64), array([ 0,  7,  3, ..., 11,  8, 10], dtype=int64), array([0, 4, 0, ..., 4, 2, 5], dtype=int64), array([2, 5, 5, ..., 6, 6, 0], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# master block to invoke pipeline\n",
    "\n",
    "# build fully qualified names for the files for saving the pipelines\n",
    "pipeline_path = get_pipeline_path()\n",
    "pipeline1_file_name = os.path.join(pipeline_path,'sc_delay_pipleline'+modifier+'.pkl')\n",
    "pipeline2_file_name = os.path.join(pipeline_path,'sc_delay_pipleline_keras_prep'+modifier+'.pkl')\n",
    "\n",
    "# define column lists:\n",
    "collist,continuouscols,textcols = def_col_lists()\n",
    "\n",
    "# create objects of the pipeline classes\n",
    "fe = fill_empty()\n",
    "ec = encode_categorical()\n",
    "pk = prep_for_keras_input()\n",
    "\n",
    "# need to implement the pipeline in two parts:\n",
    "# 1. fill empty + encode categoricals\n",
    "# 2. prep for Keras\n",
    "# because part 1 needs to be applied to the entire dataset and part 2 to the individual train, validate, and test sets\n",
    "\n",
    "\n",
    "sc_delay_pipeline = Pipeline([('fill_empty',fe),('encode_categorical',ec)])\n",
    "sc_delay_pipeline_keras_prep = Pipeline([('prep_for_keras',pk)])\n",
    "\n",
    "\n",
    "\n",
    "# provide the value for each parameter of each of the pipeline classes\n",
    "\n",
    "sc_delay_pipeline.set_params(fill_empty__collist = collist, fill_empty__continuouscols = continuouscols,\n",
    "                            fill_empty__textcols = textcols,encode_categorical__col_list = collist)\n",
    "sc_delay_pipeline_keras_prep.set_params(prep_for_keras__collist = collist,\n",
    "                            prep_for_keras__continuouscols = continuouscols,\n",
    "                            prep_for_keras__textcols = textcols)\n",
    "\n",
    "# fit the input dataset to the pipeline\n",
    "\n",
    "# first fit the first segment of pipeline on the whole dataset\n",
    "X = sc_delay_pipeline.fit_transform(merged_data)\n",
    "max_dict = ec.max_dict\n",
    "# then split dataset\n",
    "dump(sc_delay_pipeline, open(pipeline1_file_name,'wb'))\n",
    "dump(sc_delay_pipeline_keras_prep, open(pipeline2_file_name,'wb'))\n",
    "dtrain, dvalid, test = get_train_validation_test(X)\n",
    "# then apply second portion of pipeline to each subset\n",
    "\n",
    "X_train, X_train_list = get_keras_vars(dtrain)\n",
    "X_valid, X_valid_list = get_keras_vars(dvalid)\n",
    "X_test,X_test_list = get_keras_vars(test)\n",
    "\n",
    "print(\"keras variables defined\")\n",
    "print(\"X_train_list\",X_train_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and fit model <a name='modelfit' />\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to define embeddings\n",
      "textmax is 50\n",
      "through loops for cols\n",
      "through definition of non-text parts of main_l\n",
      "main_l Tensor(\"concatenate_5/Identity:0\", shape=(None, 70), dtype=float32)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "daym (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Direction (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 10)        310         daym[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 10)        50          Direction[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Route (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 10)        40          embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 10)        40          embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 10)        140         Route[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hour (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 10)        40          embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 10)        240         hour[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "month (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 10)        40          embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 10)        120         month[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "year (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20)           0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10)           0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 10)        40          embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 10)        60          year[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "day (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30)           0           concatenate[0][0]                \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 10)           0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 10)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 10)        40          embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 10)        70          day[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 40)           0           concatenate_1[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 10)           0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 10)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 10)        40          embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 50)           0           concatenate_2[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 10)           0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 10)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 60)           0           concatenate_3[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 10)           0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 70)           0           concatenate_4[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            71          concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,341\n",
      "Trainable params: 1,201\n",
      "Non-trainable params: 140\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model in Keras\n",
    "\n",
    "def get_model():\n",
    "\n",
    "\n",
    "    catinputs = {} # list of categorical inputs\n",
    "    textinputs = {} # list of text inputs\n",
    "    continputs = {} # list of continuous inputs\n",
    "    embeddings = {}\n",
    "    textembeddings = {}\n",
    "    catemb = 10 # size of categorical embeddings\n",
    "    textemb = 50 # size of text embeddings\n",
    "\n",
    "\n",
    "    print(\"about to define embeddings\")\n",
    "    collistfix = []\n",
    "    textlayerlist = []\n",
    "    inputlayerlist = []\n",
    "    i = 0\n",
    "    print(\"textmax is\",textmax)\n",
    "    # define layers for categorical columns\n",
    "    for col in collist:\n",
    "        catinputs[col] = Input(shape=[1],name=col)\n",
    "        inputlayerlist.append(catinputs[col])\n",
    "        #print(\"inputname\",inputname)\n",
    "        embeddings[col] = (Embedding(max_dict[col],catemb) (catinputs[col]))\n",
    "        # batchnorm all\n",
    "        embeddings[col] = (BatchNormalization() (embeddings[col]))\n",
    "        collistfix.append(embeddings[col])\n",
    "\n",
    "\n",
    "\n",
    "    # define layers for text columns\n",
    "    if includetext:\n",
    "        for col in textcols:\n",
    "            print(\"col\",col)\n",
    "            textinputs[col] = Input(shape=[X_train[col].shape[1]], name=col)\n",
    "            print(\"text input shape\",X_train[col].shape[1])\n",
    "            inputlayerlist.append(textinputs[col])\n",
    "            textembeddings[col] = (Embedding(textmax,textemb) (textinputs[col]))\n",
    "            textembeddings[col] = (BatchNormalization() (textembeddings[col]))\n",
    "            textembeddings[col] = Dropout(dropout_rate) ( GRU(16,kernel_regularizer=l2(l2_lambda)) (textembeddings[col]))\n",
    "            collistfix.append(textembeddings[col])\n",
    "            print(\"max in the midst\",np.max([np.max(train[col].max()), np.max(test[col].max())])+10)\n",
    "        print(\"through loops for cols\")\n",
    "\n",
    "    # define layers for continuous columns\n",
    "    for col in continuouscols:\n",
    "        continputs[col] = Input(shape=[1],name=col)\n",
    "        inputlayerlist.append(continputs[col])\n",
    "\n",
    "\n",
    "\n",
    "    # build up layers\n",
    "    # main_l = concatenate([Dropout(dropout_rate) (Flatten() (embeddings['Vehicle']) ),Dropout(dropout_rate) (Flatten() (embeddings['Direction']) )])\n",
    "    main_l = concatenate([Dropout(dropout_rate) (Flatten() (embeddings[collist[0]]) ),Dropout(dropout_rate) (Flatten() (embeddings[collist[1]]) )])\n",
    "    for cols in collist:\n",
    "        if (cols != collist[0]) & (cols != collist[1]):\n",
    "            main_l = concatenate([main_l,Dropout(dropout_rate) (Flatten() (embeddings[cols]) )])\n",
    "\n",
    "    print(\"through definition of non-text parts of main_l\")\n",
    "    if includetext:\n",
    "        for col in textcols:\n",
    "            main_l = concatenate([main_l,textembeddings[col]])\n",
    "\n",
    "    for col in continuouscols:\n",
    "        main_l = concatenate([main_l,continputs[col]])\n",
    "\n",
    "    print(\"main_l\", main_l)\n",
    "\n",
    "\n",
    "\n",
    "    # define output layer\n",
    "    output = Dense(1, activation=output_activation) (main_l)\n",
    "\n",
    "    # define model\n",
    "\n",
    "    model = Model(inputlayerlist, output)\n",
    "\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=loss_func, optimizer=optimizer, metrics=[\"accuracy\"], weighted_metrics=[\"accuracy\"])\n",
    "    # model.compile(loss=loss_func, optimizer=optimizer, metrics=[\"accuracy\", Recall(), Precision()], weighted_metrics=[\"accuracy\"])\n",
    "    \n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# output model summary\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model plot\n",
    "if save_model_plot:\n",
    "    model_plot_file = \"model_plot\"+modifier+\".png\"\n",
    "    model_plot_path = os.path.join(get_path(),model_plot_file)\n",
    "    print(\"model plot path: \",model_plot_path)\n",
    "    plot_model(model, to_file=model_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up early stopping\n",
    "def set_early_stop(es_monitor, es_mode):\n",
    "    ''' given monitoring parameter es_monitor and mode es_mode, define early stopping callback, save model callback, and \n",
    "    TensorBoard callback'''\n",
    "    # define callback for early stopping\n",
    "    callback_list = []\n",
    "    es = EarlyStopping(monitor=es_monitor, mode=es_mode, verbose=1,patience = patience_threshold)\n",
    "    callback_list.append(es)\n",
    "    model_path = get_model_path()\n",
    "    save_model_path = os.path.join(model_path,'scmodel'+modifier+\"_\"+str(experiment_number)+'.h5')\n",
    "    # define callback to save best model\n",
    "    mc = ModelCheckpoint(save_model_path, monitor=es_monitor, mode=es_mode, verbose=1, save_best_only=True)\n",
    "    callback_list.append(mc)\n",
    "    # define callback for TensorBoard\n",
    "    if tensorboard_callback:\n",
    "        tensorboard_log_dir = os.path.join(get_path(),\"tensorboard_log\",datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard = TensorBoard(log_dir= tensorboard_log_dir)\n",
    "        callback_list.append(tensorboard)\n",
    "    return(callback_list,save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text cols []\n",
      "dropout  0.0003\n",
      "L2 lambda  0.0003\n",
      "batch size  1000\n",
      "epochs 50\n",
      "learning_rate 0.001\n",
      "loss function binary_crossentropy\n",
      "output activation function hard_sigmoid\n",
      "patienc_threshold is  15\n",
      "about to define embeddings\n",
      "textmax is 50\n",
      "through loops for cols\n",
      "through definition of non-text parts of main_l\n",
      "main_l Tensor(\"concatenate_11/Identity:0\", shape=(None, 70), dtype=float32)\n",
      "Train on 1889655 samples, validate on 472414 samples\n",
      "Epoch 1/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.4030 - accuracy: 0.9050 - accuracy_1: 0.9050 - val_loss: 0.1780 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 2/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.1194 - accuracy: 0.9791 - accuracy_1: 0.9791 - val_loss: 0.1151 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 3/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.3078 - accuracy: 0.9512 - accuracy_1: 0.9512 - val_loss: 0.2097 - val_accuracy: 0.9791 - val_accuracy_1: 0.9791\n",
      "Epoch 4/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.2110 - accuracy: 0.9756 - accuracy_1: 0.9756 - val_loss: 0.1663 - val_accuracy: 0.9795 - val_accuracy_1: 0.9795\n",
      "Epoch 5/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.1368 - accuracy: 0.9795 - accuracy_1: 0.9795 - val_loss: 0.1311 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 6/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.1249 - accuracy: 0.9795 - accuracy_1: 0.9795 - val_loss: 0.1488 - val_accuracy: 0.9770 - val_accuracy_1: 0.9770\n",
      "Epoch 7/50\n",
      "1889655/1889655 [==============================] - 14s 8us/sample - loss: 0.1432 - accuracy: 0.9757 - accuracy_1: 0.9757 - val_loss: 0.1462 - val_accuracy: 0.9741 - val_accuracy_1: 0.9741\n",
      "Epoch 8/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1215 - accuracy: 0.9790 - accuracy_1: 0.9790 - val_loss: 0.1092 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 9/50\n",
      "1889655/1889655 [==============================] - 14s 7us/sample - loss: 0.4748 - accuracy: 0.9142 - accuracy_1: 0.9142 - val_loss: 0.1794 - val_accuracy: 0.9737 - val_accuracy_1: 0.9737\n",
      "Epoch 10/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1251 - accuracy: 0.9791 - accuracy_1: 0.9791 - val_loss: 0.0959 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 11/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.0990 - accuracy: 0.9795 - accuracy_1: 0.9795 - val_loss: 0.1428 - val_accuracy: 0.9790 - val_accuracy_1: 0.9790\n",
      "Epoch 12/50\n",
      "1889655/1889655 [==============================] - 16s 9us/sample - loss: 0.1646 - accuracy: 0.9706 - accuracy_1: 0.9706 - val_loss: 0.1278 - val_accuracy: 0.9796 - val_accuracy_1: 0.9796\n",
      "Epoch 13/50\n",
      "1889655/1889655 [==============================] - 18s 10us/sample - loss: 0.1245 - accuracy: 0.9795 - accuracy_1: 0.9795 - val_loss: 0.1124 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 14/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1088 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.1038 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 15/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1067 - accuracy: 0.9776 - accuracy_1: 0.9776 - val_loss: 0.2123 - val_accuracy: 0.9493 - val_accuracy_1: 0.9493\n",
      "Epoch 16/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.1371 - accuracy: 0.9724 - accuracy_1: 0.9724 - val_loss: 0.1973 - val_accuracy: 0.9582 - val_accuracy_1: 0.9582\n",
      "Epoch 17/50\n",
      "1889655/1889655 [==============================] - 16s 9us/sample - loss: 0.1411 - accuracy: 0.9758 - accuracy_1: 0.9758 - val_loss: 0.1099 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 18/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1316 - accuracy: 0.9763 - accuracy_1: 0.9763 - val_loss: 0.1391 - val_accuracy: 0.9790 - val_accuracy_1: 0.9790\n",
      "Epoch 19/50\n",
      "1889655/1889655 [==============================] - 16s 9us/sample - loss: 0.1142 - accuracy: 0.9796 - accuracy_1: 0.9796 - val_loss: 0.1396 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 20/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.1089 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.0995 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 21/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.1546 - accuracy: 0.9688 - accuracy_1: 0.9688 - val_loss: 0.1378 - val_accuracy: 0.9783 - val_accuracy_1: 0.9783\n",
      "Epoch 22/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1236 - accuracy: 0.9794 - accuracy_1: 0.9794 - val_loss: 0.1141 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 23/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1177 - accuracy: 0.9789 - accuracy_1: 0.9789 - val_loss: 0.1694 - val_accuracy: 0.9768 - val_accuracy_1: 0.9768\n",
      "Epoch 24/50\n",
      "1889655/1889655 [==============================] - 16s 9us/sample - loss: 0.1641 - accuracy: 0.9641 - accuracy_1: 0.9641 - val_loss: 0.1408 - val_accuracy: 0.9769 - val_accuracy_1: 0.9769\n",
      "Epoch 25/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.1198 - accuracy: 0.9794 - accuracy_1: 0.9794 - val_loss: 0.1072 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 26/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1163 - accuracy: 0.9778 - accuracy_1: 0.9778 - val_loss: 0.1067 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 27/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.0981 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.0933 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 28/50\n",
      "1889655/1889655 [==============================] - 18s 9us/sample - loss: 0.1049 - accuracy: 0.9795 - accuracy_1: 0.9795 - val_loss: 0.0992 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 29/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.0939 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.0886 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 30/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.0883 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.0856 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 31/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.1017 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.0969 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 32/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1636 - accuracy: 0.9709 - accuracy_1: 0.9709 - val_loss: 0.1284 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 33/50\n",
      "1889655/1889655 [==============================] - 17s 9us/sample - loss: 0.1147 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.1039 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 34/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.1276 - accuracy: 0.9771 - accuracy_1: 0.9771 - val_loss: 0.1011 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 35/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.0939 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.0882 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 36/50\n",
      "1889655/1889655 [==============================] - 18s 10us/sample - loss: 0.0896 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.0900 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 37/50\n",
      "1889655/1889655 [==============================] - 21s 11us/sample - loss: 0.4643 - accuracy: 0.9469 - accuracy_1: 0.9469 - val_loss: 2.4660 - val_accuracy: 0.8016 - val_accuracy_1: 0.8016\n",
      "Epoch 38/50\n",
      "1889655/1889655 [==============================] - 18s 10us/sample - loss: 2.3509 - accuracy: 0.8219 - accuracy_1: 0.8219 - val_loss: 2.2392 - val_accuracy: 0.8278 - val_accuracy_1: 0.8278\n",
      "Epoch 39/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 1.1260 - accuracy: 0.8814 - accuracy_1: 0.8814 - val_loss: 0.2343 - val_accuracy: 0.9646 - val_accuracy_1: 0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "1889655/1889655 [==============================] - 16s 8us/sample - loss: 0.2053 - accuracy: 0.9758 - accuracy_1: 0.9758 - val_loss: 0.1931 - val_accuracy: 0.9793 - val_accuracy_1: 0.9793\n",
      "Epoch 41/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.1998 - accuracy: 0.9751 - accuracy_1: 0.9751 - val_loss: 0.1825 - val_accuracy: 0.9794 - val_accuracy_1: 0.9794\n",
      "Epoch 42/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.1765 - accuracy: 0.9796 - accuracy_1: 0.9796 - val_loss: 0.1720 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 43/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.1701 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.1689 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 44/50\n",
      "1889655/1889655 [==============================] - 14s 7us/sample - loss: 0.1702 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.1700 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 45/50\n",
      "1889655/1889655 [==============================] - 14s 8us/sample - loss: 0.1684 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.1674 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n",
      "Epoch 46/50\n",
      "1889655/1889655 [==============================] - 15s 8us/sample - loss: 0.1658 - accuracy: 0.9797 - accuracy_1: 0.9797 - val_loss: 0.1661 - val_accuracy: 0.9797 - val_accuracy_1: 0.9797\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"text cols\",textcols)\n",
    "print(\"dropout \",dropout_rate)\n",
    "print(\"L2 lambda \",l2_lambda)\n",
    "print(\"batch size \",batch_size)\n",
    "print(\"epochs\",epochs)\n",
    "print(\"learning_rate\",learning_rate)\n",
    "print(\"loss function\",loss_func)\n",
    "print(\"output activation function\",output_activation)\n",
    "print(\"patience_threshold is \",patience_threshold)\n",
    "# get definitions\n",
    "callback_list, save_model_path = set_early_stop(es_monitor, es_mode)\n",
    "model = get_model()\n",
    "if early_stop:\n",
    "       modelfit = model.fit(X_train_list, dtrain.target, epochs=epochs, batch_size=batch_size\n",
    "        , validation_data=(X_valid_list, dvalid.target), class_weight = {0 : zero_weight, 1: one_weight}, verbose=1,callbacks=callback_list)\n",
    "else:\n",
    "    modelfit = model.fit(X_train_list, dtrain.target, epochs=epochs, batch_size=batch_size\n",
    "         , validation_data=(X_valid_list, dvalid.target), class_weight = {0 : zero_weight, 1: one_weight}, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model elements explicitly if not saved as part of early_stop\n",
    "if early_stop == False:\n",
    "    model_json = model.to_json()\n",
    "    model_path = get_model_path()\n",
    "    with open(os.path.join(model_path,'model'+modifier+'.json'), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(os.path.join(model_path,'scweights'+modifier+'.h5'))\n",
    "    save_model_path = os.path.join(model_path,'scmodel'+modifier+'.h5')\n",
    "    model.save(save_model_path,save_format='h5')\n",
    "    # no early stop, so make current model saved_model\n",
    "    saved_model = model\n",
    "    print(\"Saved model, weights to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model(save_model_path)\n",
    "if len(saved_model.metrics_names) == 2:\n",
    "    # saved_model.evaluate returns ['loss', 'acc']\n",
    "    _, train_acc = saved_model.evaluate(X_train_list, dtrain.target, verbose=0)\n",
    "    _, test_acc = saved_model.evaluate(X_test, test.target, verbose=0)\n",
    "else:\n",
    "    # saved_model.evaluate returns ['loss', 'accuracy', 'accuracy_1']\n",
    "    _, train_acc,_ = saved_model.evaluate(X_train_list, dtrain.target, verbose=0)\n",
    "    _, test_acc,_ = saved_model.evaluate(X_test, test.target, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "#print('Train recall: %.3f, Train precision: %.3f, Train acc1: %.3f' % (train_recall,train_precision,train_acc1))\n",
    "#print('Test recall: %.3f, Test precision: %.3f, Test acc1: %.3f' % (test_recall,test_precision,test_acc1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model and weights <a name='reload' />\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if presaved == True:\n",
    "    batch_size = 1000\n",
    "    epochs = 1\n",
    "    modelfit2 = loaded_model.fit(X_train_list, dtrain.target, epochs=epochs, batch_size=batch_size\n",
    "         , validation_data=(X_valid_list, dvalid.target), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and renderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to parse and manipulate dates in the style of the input CSVs\n",
    "\n",
    "def create_date(year,month):\n",
    "    outdate = datetime.date(year,month,15)\n",
    "    return(outdate)\n",
    "\n",
    "def parse_bic_date(bic_date_in):\n",
    "    year = int(bic_date_in[0:4])\n",
    "    month = int(bic_date_in[-2:])\n",
    "    return(year,month)\n",
    "\n",
    "def create_date_from_bic(bic_date_in):\n",
    "    yr,mth = parse_bic_date(bic_date_in)\n",
    "    retdate = create_date(yr,mth)\n",
    "    return retdate\n",
    "\n",
    "def get_datecomp_from_csvdate (csv_date):\n",
    "    day = int(csv_date[0:2])\n",
    "    # month_number = datetime.datetime.strptime(month_name, '%b').month\n",
    "    month = datetime.datetime.strptime(csv_date[3:6], '%b').month\n",
    "    year = int('20'+csv_date[-2:])\n",
    "    # year = int(csv_date[-2:])\n",
    "    return (year,month,day)\n",
    "\n",
    "def get_date_from_csvdate (csv_date):\n",
    "    day = int(csv_date[0:2])\n",
    "    # month_number = datetime.datetime.strptime(month_name, '%b').month\n",
    "    month = datetime.datetime.strptime(csv_date[3:6], '%b',coerce=True).month\n",
    "    year = int('20'+csv_date[-2:])\n",
    "    # year = int(csv_date[-2:])\n",
    "    return (date(year,month,day))\n",
    "\n",
    "def get_year_from_csvdate (csv_date):\n",
    "    year = int('20'+csv_date[-2:])\n",
    "    return (year)\n",
    "\n",
    "def get_month_from_csvdate (csv_date):\n",
    "    month = datetime.datetime.strptime(csv_date[3:6], '%b',coerce=True).month\n",
    "    return (month)\n",
    "\n",
    "def get_day_from_csvdate (csv_date):\n",
    "    day = int(csv_date[0:2])\n",
    "    return (day)\n",
    "\n",
    "def get_weekday (date):\n",
    "    return(date.weekday())\n",
    "\n",
    "# pd.to_datetime(x, coerce=True)\n",
    "\n",
    "def validatedate(csv_text):\n",
    "    try:\n",
    "        datetime.datetime.strptime(csv_date[3:6], '%b')\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Incorrect data format, should be YYYY-MM-DD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X_test_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test \", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on training set\n",
    "\n",
    "preds = saved_model.predict(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"predict\"] = preds\n",
    "test.predict[:5]\n",
    "if verboseout:\n",
    "    test.predict.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verboseout:\n",
    "    test.predict.hist(bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rounded predictions\n",
    "test[\"predround\"] = preds.round().astype(int)\n",
    "test.predround[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test target 0\",test[test['target']==0].shape[0])\n",
    "print(\"test target 1\",test[test['target']==1].shape[0])\n",
    "print(\"test predround 0\",test[test['predround']==0].shape[0])\n",
    "print(\"test predround 1\",test[test['predround']==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get delta between predictions on training set and actual training target values\n",
    "# hand calculate accuracy on training set as ratio of (total training samples - wrong training predictions)/total training samples\n",
    "\n",
    "deltatr = abs(test.target[:100000] - test.predround[:100000])\n",
    "deltatr[:50]\n",
    "print(deltatr.sum())\n",
    "print(\"percentage correct test\")\n",
    "print((len(deltatr) - deltatr.sum())/len(deltatr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values for validation X values\n",
    "# X_valid, dvalid.target\n",
    "predval = model.predict(X_valid, batch_size=batch_size)\n",
    "dvalid[\"predround\"] = predval.round().astype(int)\n",
    "dvalid[\"predict\"] = predval\n",
    "#print(type(deltaval))\n",
    "#print(len(deltaval))\n",
    "dvalid.predict[:5]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verboseout:\n",
    "    dvalid.predict.hist(bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand calculation of proportion correct guesses in validation set\n",
    "\n",
    "dvalid[\"deltaval\"] = abs(dvalid.target - dvalid.predround)\n",
    "print(dvalid[\"deltaval\"][:10])\n",
    "print(dvalid[\"deltaval\"].sum())\n",
    "# print(\"percentage correct\")\n",
    "# print((len(deltaval) - deltaval.sum())/len(deltaval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand calculation of proportion correct guesses in validation set\n",
    "\n",
    "test[\"deltaval\"] = abs(test.target - test.predround)\n",
    "print(test[\"deltaval\"][:10])\n",
    "print(test[\"deltaval\"].sum())\n",
    "# print(\"percentage correct\")\n",
    "# print((len(deltaval) - deltaval.sum())/len(deltaval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of dataframe with wrong guesses\n",
    "# k1 = df.loc[(df.Product == p_id)\n",
    "dvalidwrong = dvalid.loc[(dvalid.deltaval == 1)]\n",
    "dvalidright = dvalid.loc[(dvalid.deltaval == 0)]\n",
    "dvalidwrong.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of dataframe with wrong guesses\n",
    "# k1 = df.loc[(df.Product == p_id)\n",
    "testwrong = test.loc[(test.deltaval == 1)]\n",
    "testright = test.loc[(test.deltaval == 0)]\n",
    "testwrong.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dvalid.hist(range = (0,5))\n",
    "# apar_ds.Time_to_relief.hist(range = (0,5))\n",
    "dvalid.predict.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dvalid.hist(range = (0,5))\n",
    "# apar_ds.Time_to_relief.hist(range = (0,5))\n",
    "test.predict.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get delta between predictions on training set and actual training target values\n",
    "# hand calculate accuracy on training set as ratio of (total training samples - wrong training predictions)/total training samples\n",
    "\n",
    "deltatr = abs(dvalid.target[:100000] - dvalid.predround[:100000])\n",
    "deltatr[:50]\n",
    "print(deltatr.sum())\n",
    "print(\"percentage correct validate\")\n",
    "print((len(deltatr) - deltatr.sum())/len(deltatr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart accuracy and loss for train and validation sets\n",
    "\n",
    "print(modelfit.history.keys())\n",
    "#  acc\n",
    "plt.plot(modelfit.history['accuracy'])\n",
    "plt.plot(modelfit.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# Loss\n",
    "plt.plot(modelfit.history['loss'])\n",
    "plt.plot(modelfit.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix <a name='confusionmatrix' />\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cfmap=metrics.confusion_matrix(y_true=test['target'],  # True labels\n",
    "                         y_pred=test[\"predround\"])\n",
    "\n",
    "label = [\"0\", \"1\"]\n",
    "sns.heatmap(cfmap, annot = True, xticklabels = label, yticklabels = label)\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.title(\"Confusion Matrix for streetcar delay prediction (weighted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle submission that was used as input for this notebook\n",
    "https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook shows methods for dealing with structured data in the context of a neural network.\n",
    "\n",
    "# Author\n",
    "\n",
    "Mark Ryan is a manager at Intact Insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "try_tf2",
   "language": "python",
   "name": "try_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
